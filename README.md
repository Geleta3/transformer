# Transformer

 This repository contains a simplified source code for self-attention and cross attention, namely transformer. 
 
 Here are some tools I recommend to have good understanding about transformers:
 
 [Illustrated-guide-to-transformers-step-by-step-explanation](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)
 
 [Video tutorial - 1](https://www.youtube.com/watch?v=4Bdc55j80l8&pp=ygUUdHJhbnNmb3JtZXIgdHV0b3JpYWw%3D) 
 
 [Video tutorial - 2](https://www.youtube.com/watch?v=XSSTuhyAmnI&pp=ygUUdHJhbnNmb3JtZXIgdHV0b3JpYWw%3D) 
 
 Reference:
 * [Attention is all you need] https://arxiv.org/abs/1706.03762
