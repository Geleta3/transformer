# Transformer

 This repository contains a simplified source code for self-attention and cross attention, namely transformer. 
 
 Here are some tools I recommend to have good understanding about transformers:
 
 [1](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0) illustrated-guide-to-transformers-step-by-step-explanation
 
 [2](https://www.youtube.com/watch?v=4Bdc55j80l8&pp=ygUUdHJhbnNmb3JtZXIgdHV0b3JpYWw%3D) Video tutorial - 1
 
 [3](https://www.youtube.com/watch?v=XSSTuhyAmnI&pp=ygUUdHJhbnNmb3JtZXIgdHV0b3JpYWw%3D) Video tutorial - 2
 
 Reference:
 * [Attention is all you need] https://arxiv.org/abs/1706.03762
