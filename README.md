# self-attention
 This repository contains a simplified source code for self-attention and cross attention, namely transformer
